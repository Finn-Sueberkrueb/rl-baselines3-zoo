# Tuned
MountainCarContinuous-v0:
  n_timesteps: !!float 50000
  policy: 'MlpPolicy'
  learning_rate: !!float 3e-4
  buffer_size: 50000
  batch_size: 512
  ent_coef: 0.1
  train_freq: 32
  gradient_steps: 32
  gamma: 0.9999
  tau: 0.01
  learning_starts: 0
  use_sde: True
  munchausen_scaling: 0.9
  munchausen_clipping_low: -1
  munchausen_clipping_high: 1
  munchausen_mode: 'default' # ['no_clipping', 'fix_scale', 'shift', 'dynamicshift_target_entropy', 'dynamicshift', 'dynamicshift_max', 'dynamicshift_hyper', 'dynamicmean_hyper', 'dynamicshift_median', 'dynamicshift_normalized', 'default']
  dynamicshift_hyperparameter: 0.0
  policy_kwargs: "dict(log_std_init=-3.67, net_arch=[64, 64])"

Pendulum-v0:
  # callback:
  #   - utils.callbacks.ParallelTrainCallback
  n_timesteps: 20000
  policy: 'MlpPolicy'
  learning_rate: !!float 1e-3
  use_sde: True
  train_freq: [1, "episode"]
  gradient_steps: -1
  policy_kwargs: "dict(log_std_init=-2, net_arch=[64, 64])"

LunarLanderContinuous-v2:
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  batch_size: 256
  learning_rate: lin_7.3e-4
  buffer_size: 1000000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.99
  tau: 0.01
  train_freq: 1
  gradient_steps: 1
  learning_starts: 10000
  munchausen_scaling: 0.9
  munchausen_clipping_low: -1
  munchausen_clipping_high: 1
  munchausen_mode: 'default' # ['no_clipping', 'fix_scale', 'shift', 'dynamicshift_target_entropy', 'dynamicshift', 'dynamicshift_max', 'dynamicshift_hyper', 'dynamicmean_hyper', 'dynamicshift_median', 'dynamicshift_normalized', 'default']
  dynamicshift_hyperparameter: 0.0
  policy_kwargs: "dict(net_arch=[400, 300])"


# Tuned
BipedalWalker-v3:
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  buffer_size: 300000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.98
  tau: 0.02
  train_freq: 64
  gradient_steps: 64
  learning_starts: 10000
  use_sde: True
  munchausen_scaling: 0.9
  munchausen_clipping_low: -1
  munchausen_clipping_high: 1
  munchausen_mode: 'default' # ['no_clipping', 'fix_scale', 'shift', 'dynamicshift_target_entropy', 'dynamicshift', 'dynamicshift_max', 'dynamicshift_hyper', 'dynamicmean_hyper', 'dynamicshift_median', 'dynamicshift_normalized', 'default']
  dynamicshift_hyperparameter: 0.0
  policy_kwargs: "dict(log_std_init=-3, net_arch=[400, 300])"

# Almost tuned
BipedalWalkerHardcore-v3:
  n_timesteps: !!float 1e7
  policy: 'MlpPolicy'
  learning_rate: lin_7.3e-4
  buffer_size: 1000000
  batch_size: 256
  ent_coef: 0.005
  gamma: 0.99
  tau: 0.01
  train_freq: 1
  gradient_steps: 1
  learning_starts: 10000
  munchausen_scaling: 0.9
  munchausen_clipping_low: -1
  munchausen_clipping_high: 1
  munchausen_mode: 'default' # ['no_clipping', 'fix_scale', 'shift', 'dynamicshift_target_entropy', 'dynamicshift', 'dynamicshift_max', 'dynamicshift_hyper', 'dynamicmean_hyper', 'dynamicshift_median', 'dynamicshift_normalized', 'default']
  dynamicshift_hyperparameter: 0.0
  policy_kwargs: "dict(net_arch=[400, 300])"

# === Bullet envs ===

# Tuned
HalfCheetahBulletEnv-v0:
  # env_wrapper:
  #   - sb3_contrib.common.wrappers.TimeFeatureWrapper
  #   - utils.wrappers.DelayedRewardWrapper:
  #       delay: 10
  #   - utils.wrappers.HistoryWrapper:
  #       horizon: 10
  # env_wrapper: sb3_contrib.common.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  buffer_size: 300000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.98
  tau: 0.02
  train_freq: 64
  gradient_steps: 64
  learning_starts: 10000
  replay_buffer_kwargs: "dict(handle_timeout_termination=True)"
  use_sde: True
  munchausen_scaling: 0.9
  munchausen_clipping_low: -1
  munchausen_clipping_high: 1
  munchausen_mode: 'default' # ['no_clipping', 'fix_scale', 'shift', 'dynamicshift_target_entropy', 'dynamicshift', 'dynamicshift_max', 'dynamicshift_hyper', 'dynamicmean_hyper', 'dynamicshift_median', 'dynamicshift_normalized', 'default']
  dynamicshift_hyperparameter: 0.0
  policy_kwargs: "dict(log_std_init=-3, net_arch=[400, 300])"

# Tuned
AntBulletEnv-v0:
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  buffer_size: 300000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.98
  tau: 0.02
  train_freq: 64
  gradient_steps: 64
  learning_starts: 10000
  use_sde: True
  munchausen_scaling: 0.9
  munchausen_clipping_low: -1
  munchausen_clipping_high: 1
  munchausen_mode: 'default' # ['no_clipping', 'fix_scale', 'shift', 'dynamicshift_target_entropy', 'dynamicshift', 'dynamicshift_max', 'dynamicshift_hyper', 'dynamicmean_hyper', 'dynamicshift_median', 'dynamicshift_normalized', 'default']
  dynamicshift_hyperparameter: 0.0
  policy_kwargs: "dict(log_std_init=-3, net_arch=[400, 300])"

HopperBulletEnv-v0:
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  learning_rate: lin_7.3e-4
  buffer_size: 300000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.98
  tau: 0.02
  train_freq: 64
  gradient_steps: 64
  learning_starts: 10000
  use_sde: True
  munchausen_scaling: 0.9
  munchausen_clipping_low: -1
  munchausen_clipping_high: 1
  munchausen_mode: 'default' # ['no_clipping', 'fix_scale', 'shift', 'dynamicshift_target_entropy', 'dynamicshift', 'dynamicshift_max', 'dynamicshift_hyper', 'dynamicmean_hyper', 'dynamicshift_median', 'dynamicshift_normalized', 'default']
  dynamicshift_hyperparameter: 0.0
  policy_kwargs: "dict(log_std_init=-3, net_arch=[400, 300])"

Walker2DBulletEnv-v0:
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  learning_rate: lin_7.3e-4
  buffer_size: 300000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.98
  tau: 0.02
  train_freq: 64
  gradient_steps: 64
  learning_starts: 10000
  use_sde: True
  munchausen_scaling: 0.9
  munchausen_clipping_low: -1
  munchausen_clipping_high: 1
  munchausen_mode: 'default' # ['no_clipping', 'fix_scale', 'shift', 'dynamicshift_target_entropy', 'dynamicshift', 'dynamicshift_max', 'dynamicshift_hyper', 'dynamicmean_hyper', 'dynamicshift_median', 'dynamicshift_normalized', 'default']
  dynamicshift_hyperparameter: 0.0
  policy_kwargs: "dict(log_std_init=-3, net_arch=[400, 300])"


# Tuned
ReacherBulletEnv-v0:
  n_timesteps: !!float 3e5
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  buffer_size: 300000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.98
  tau: 0.02
  train_freq: 64
  gradient_steps: 64
  learning_starts: 10000
  use_sde: True
  munchausen_scaling: 0.9
  munchausen_clipping_low: -1
  munchausen_clipping_high: 1
  munchausen_mode: 'default' # ['no_clipping', 'fix_scale', 'shift', 'dynamicshift_target_entropy', 'dynamicshift', 'dynamicshift_max', 'dynamicshift_hyper', 'dynamicmean_hyper', 'dynamicshift_median', 'dynamicshift_normalized', 'default']
  dynamicshift_hyperparameter: 0.0
  policy_kwargs: "dict(log_std_init=-3, net_arch=[400, 300])"

